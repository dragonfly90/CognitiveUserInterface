<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">

<HTML>

<HEAD>
  <LINK rel=stylesheet HREF="../../style/style.css">
  <TITLE></TITLE>
</HEAD>

<BODY>

  <div id=pageheader>
    <div id=logo><a href="http://www.iccm2015.org"><img src="../../style/logo.png" title="ICCM 2015 logo" alt="ICCM 2015 logo"></a></div>
    <div id=title></div>
    <br clear=both>
    <div id=menubar>
      <ul>
	
	<li><a href="../../index.html">Table of Contents</a>
      </ul>
    </div>
  </div>

  <div id=pagebody>

    

	<h1>Exploration-Exploitation in a Contextual Multi-Armed Bandit Task</h1>

    
	  <ul>
    	    <li>Eric Schulz, <em>University College London, London, London, United Kingdom</em>
    	    <li>Emmanouil Konstantinidis, <em>Carnegie Mellon University</em>
    	    <li>Maarten Speekenbrink, <em>University College London</em>
                                                    	  </ul>

    

	<h2>Abstract</h2>

          <p id=abstract>We introduce the Contextual Multi-Armed Bandit task as a method to
      assess decision making in uncertain environments and test how participants behave
      in this task. Within an experimental paradigm named ``Mining in Space'',
      participants see 4 different planets that are described by 3 different binary
      elements (the context) and then have to decide on which planet they want to mine
      (which arm to play). We find that participants adapt their decisions to the
      context well and can best be described by a Contextual Gaussian Process algorithm
      that probability matches according to expected outcomes. We conclude that humans
      are well-adapted to contextualized bandit problems even in potentially
      non-stationary environments through probability matching, a heuristic that used
      to be described as biased behavior. We argue that Contextual Bandit problems can
      provide further insight into how people make decisions in real world
      scenarios.</p>

    





	  <ul>


	    <li id=files>The Paper: <a href="paper0031.pdf">Exploration-Exploitation in a Contextual Multi-Armed Bandit Task</a>


	  </ul>

	<p><br><a href="../../index.html">Back</a>

  </div>

  <div id=pagefooter>
  </div>

</BODY>
</HTML>

